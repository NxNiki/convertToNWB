{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An initial attempt at NWB conversion for NeuraLynx data following \"manual\" conversion described in https://pynwb.readthedocs.io/en/stable/tutorials/domain/ecephys.html .  Unlike the example(s) there I (Yarik) was trying to identify levels of data and metadata to consider, and also to store them across multiple .nwb files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pynwb\n",
    "from datetime import datetime\n",
    "from dateutil.tz import tzlocal\n",
    "from pynwb import NWBFile\n",
    "from conv.io import MovieParadigm\n",
    "import os.path as op\n",
    "import re\n",
    "import glob\n",
    "\n",
    "import neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Volumes/DATA/NLData/D570/EXP5_Movie_24_Sleep/2024-01-27_00-01-35/GA1-REC1.ncs',\n",
       " '/Volumes/DATA/NLData/D570/EXP5_Movie_24_Sleep/2024-01-27_00-01-35/GA1-REC1_0002.ncs',\n",
       " '/Volumes/DATA/NLData/D570/EXP5_Movie_24_Sleep/2024-01-27_00-01-35/GA1-REC1_0003.ncs',\n",
       " '/Volumes/DATA/NLData/D570/EXP5_Movie_24_Sleep/2024-01-27_00-01-35/GA1-REC1_0004.ncs',\n",
       " '/Volumes/DATA/NLData/D570/EXP5_Movie_24_Sleep/2024-01-27_00-01-35/GA1-REC1_0005.ncs',\n",
       " '/Volumes/DATA/NLData/D570/EXP5_Movie_24_Sleep/2024-01-27_00-01-35/GA1-REC1_0006.ncs']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# session_data = '/Users/XinNiuAdmin/Documents/NWBTest/D550/EXP1_Screening/2022-03-10_13-34-18'\n",
    "\n",
    "directory_path = '/Volumes/DATA/NLData/D570/EXP5_Movie_24_Sleep/2024-01-27_00-01-35/'\n",
    "# Define the pattern\n",
    "pattern = 'G[A-D][1-9]*.ncs'\n",
    "\n",
    "# Use glob to find files matching the pattern in the specified directory\n",
    "session_data = glob.glob(directory_path + pattern) \n",
    "session_data = session_data[0:6]\n",
    "session_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common lab wide metadata\n",
    "lab_metadata = dict(\n",
    "    lab=\"CNL\",\n",
    "    institution=\"UCLA Health\",\n",
    "    keywords=[\"DANDI Pilot\"], # arbitrary, so let's promote!\n",
    ")\n",
    "# Experiment specific one\n",
    "experiment_metadata = dict(\n",
    "    experimenter=\"Xin Niu <michaelniki1988@gmail.com>\",  # Let's see if nwb can swallow such a record ;)\n",
    "    experiment_description=\"movie paradigm\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralynxIO: \n",
      "nb_block: 1\n",
      "nb_segment:  [328]\n",
      "signal_streams: [Stream (rate,#packet,t0): (32000.0, 450001, 1706336056470695) (chans: 1), Stream (rate,#packet,t0): (32000.0, 450001, 1706328856470711) (chans: 1), Stream (rate,#packet,t0): (32000.0, 450001, 1706321656470695) (chans: 1), Stream (rate,#packet,t0): (32000.0, 450001, 1706314456470711) (chans: 1), Stream (rate,#packet,t0): (32000.0, 50276, 1706343256470710) (chans: 1)]\n",
      "signal_channels: [GA1-REC1, GA1-REC1, GA1-REC1, GA1-REC1, GA1-REC1]\n",
      "spike_channels: []\n",
      "event_channels: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a reader\n",
    "reader = neo.io.NeuralynxIO(include_filename=session_data) # TODO: newer version should support: , keep_original_times=True)\n",
    "reader.parse_header()\n",
    "print(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seg = reader.read_segment()\n",
    "seg = reader.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subject_id': 'D570',\n",
       " 'experiment': 'EXP5_Movie_24_Sleep',\n",
       " 'date': '2024-01-27_00-01-35'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "filename_metadata = re.match(\n",
    "    r'.*/(?P<subject_id>D\\d+)/(?P<experiment>EXP\\d+_[\\w\\d]+)/(?P<date>\\d{4}-\\d{2}-\\d{2}_\\d{2}-\\d{2}-\\d{2})/',\n",
    "    directory_path,\n",
    ").groupdict()\n",
    "assert filename_metadata\n",
    "filename_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Those time stamps are in sub-second and not the one we would want to the \"session time\"\n",
    "# time.gmtime(reader.get_event_timestamps()[0][0])\n",
    "# TODO: figure out where in this \n",
    "# TODO: figure out what those timestamps in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subject_id': 'D570', 'weight': '', 'age': 'TODO', 'species': '', 'sex': 'female', 'date_of_birth': datetime.datetime(2024, 3, 21, 19, 17, 48, 251807, tzinfo=tzlocal())}\n"
     ]
    }
   ],
   "source": [
    "# Metadata which is likely to come from data files and \"promotion\" metadata records\n",
    "\n",
    "# Most likely many could be parsed from the filenames which are likely to encode some of it\n",
    "# So \"heuristical\" converter could establish metadata harvesting from the filenames\n",
    "\n",
    "#\n",
    "# Session specific\n",
    "session_metadata = dict(\n",
    "    session_id=\"%(subject_id)s-%(date)s\" % filename_metadata,\n",
    "    session_description=\"Extracellular ephys recording in the left hemisphere of the nucleus accumbens\",  # args[0] in nwbfile\n",
    "    session_start_time=datetime.now(tzlocal()), # TEMP  # args[2] in nwbfile; TODO needs to be datetime\n",
    ")\n",
    "subject_metadata = dict(\n",
    "    subject_id=filename_metadata['subject_id'],\n",
    "    weight='',\n",
    "    age=\"TODO\",  # duplicate with session_start_time and date_of_birth but why not?\n",
    "    species='',\n",
    "    sex=\"female\",\n",
    "#     hemisphere=metadata_keys['ExpKeys.hemisphere'],\n",
    "#     depth=metadata_keys['ExpKeys.probeDepth'],\n",
    "#     region=metadata_keys['ExpKeys.target'],\n",
    "    date_of_birth=datetime.now(tzlocal()), # TEMP: TODO\n",
    ")\n",
    "surgery_metadata = dict(\n",
    "    surgery=\"Headbar on xx/xx/2020, craniotomy over right hemisphere on xx/xx/2020, craniotomy over left hemisphere on xx/xx/2020. All surgeries performed by JG.\"\n",
    ")\n",
    "# Actually probably only \"identifier\" should be file specific, the rest common across files\n",
    "# we would like to produce: separate for .ncs, .ntt, behavioral metadata, etc\n",
    "file_metadata = dict(\n",
    "    source_script=\"somescript-not-clear-whyneeds to be not empty if file_name is provided\",\n",
    "    source_script_file_name=\"TODO\", # __file__,\n",
    ")\n",
    "\n",
    "# common filename prefix - let's mimic DANDI filenaming convention right away\n",
    "filename_prefix = \"sub-{subject_id}_ses-{session_id}\".format(**subject_metadata, **session_metadata)\n",
    "\n",
    "# the rest will be specific to the corresponding file. E.g. we will have separate\n",
    "#  - `_probe-<name>_ecephys.nwb` (from each .ncs) - contineous data from each tetrode. probably chunked and compressed\n",
    "#  - `_???_ecephys.nwb` (from each .ntt) - spike detected windowed data. \n",
    "#  - `_behav.mpg` + `_behav.nwb` - video recording and metadata (including those .png?) for behavior component within experiment recording session\n",
    "# Pretty much we need to establish a framework where EVERY file present would be\n",
    "# provided\n",
    "\n",
    "print(subject_metadata)\n",
    "\n",
    "# TODO add event labels to metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'session_id': 'D570-2024-01-27_00-01-35',\n",
       " 'session_description': 'Extracellular ephys recording in the left hemisphere of the nucleus accumbens',\n",
       " 'session_start_time': datetime.datetime(2024, 3, 21, 19, 17, 48, 251652, tzinfo=tzlocal())}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-D570_ses-D570-2024-01-27_00-01-35_TODO\n"
     ]
    }
   ],
   "source": [
    "# Such NWBFile will be created for each separate file, and then fill up with the corresponding\n",
    "#\n",
    "filename_suffix = \"TODO\"\n",
    "nwbfile = NWBFile(\n",
    "    identifier=\"{}_{}\".format(filename_prefix, filename_suffix), # args[1] in nwbfile, may be just UUID? not sure why user has to provide it really\n",
    "    subject=pynwb.file.Subject(**subject_metadata),\n",
    "    **lab_metadata,\n",
    "    **experiment_metadata,\n",
    "    **session_metadata,\n",
    "    **surgery_metadata,\n",
    "    **file_metadata,)\n",
    "\n",
    "print(nwbfile.identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'unit_channels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m device \u001b[38;5;241m=\u001b[39m nwbfile\u001b[38;5;241m.\u001b[39mcreate_device(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicro probe\u001b[39m\u001b[38;5;124m'\u001b[39m, description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, manufacturer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNeuraLynx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# for each channel on the probe\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chl \u001b[38;5;129;01min\u001b[39;00m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43munit_channels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m      7\u001b[0m     \n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# get tetrode id\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     tetrode \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(?<=TT)(.*?)(?=#)\u001b[39m\u001b[38;5;124m'\u001b[39m, chl[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     10\u001b[0m     electrode_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtetrode\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m tetrode\n",
      "\u001b[0;31mKeyError\u001b[0m: 'unit_channels'"
     ]
    }
   ],
   "source": [
    "# add electrode metadata\n",
    "# create probe device\n",
    "device = nwbfile.create_device(name='micro probe', description='', manufacturer='NeuraLynx')\n",
    "\n",
    "# for each channel on the probe\n",
    "for chl in reader.header['unit_channels']:\n",
    "    \n",
    "    # get tetrode id\n",
    "    tetrode = re.search('(?<=TT)(.*?)(?=#)', chl[0]).group(0)\n",
    "    electrode_name = 'tetrode' + tetrode\n",
    "    \n",
    "    # get channel id\n",
    "    channel = re.search('(?<=#)(.*?)(?=#)', chl[0]).group(0)\n",
    "           \n",
    "    if electrode_name not in nwbfile.electrode_groups: # make tetrode if does not exist\n",
    "    \n",
    "        description = electrode_name\n",
    "        location = metadata_keys['ExpKeys.hemisphere'] + ' ' + metadata_keys['ExpKeys.target'] + ' ' + \\\n",
    "            '(' + metadata_keys['ExpKeys.probeDepth'] + ' um)'\n",
    "\n",
    "        electrode_group = nwbfile.create_electrode_group(electrode_name,\n",
    "                                                         description=description,\n",
    "                                                         location=location,\n",
    "                                                         device=device)\n",
    "        \n",
    "    # add channel to tetrode\n",
    "    nwbfile.add_electrode(id=int(channel),\n",
    "                          x=-1.2, y=float(metadata_keys['ExpKeys.probeDepth']), z=-1.5,\n",
    "                          location=metadata_keys['ExpKeys.target'], filtering='none',\n",
    "                          imp = 0.0, group=nwbfile.electrode_groups[electrode_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append data from different segments\n",
    "\n",
    "spk_all = []\n",
    "wv_all = []\n",
    "csc_all_mag = []\n",
    "csc_all_time = [];\n",
    "beh_all = []\n",
    "\n",
    "for s in range(reader.header['nb_segment'][0]):\n",
    "#     for i, chl in enumerate(reader.header['unit_channels']):\n",
    "    if s is 0:\n",
    "        for i, chl in enumerate(seg[0].segments[s].spiketrains):\n",
    "            spk_all.append([seg[0].segments[s].spiketrains[i].times])\n",
    "            \n",
    "        csc_all_mag = seg[0].segments[s].analogsignals[0].magnitude\n",
    "        csc_all_time = seg[0].segments[s].analogsignals[0].times\n",
    "        \n",
    "        for i, chl in enumerate(seg[0].segments[s].events):\n",
    "            beh_all.append([seg[0].segments[s].events[i].times])\n",
    "            \n",
    "    else:\n",
    "        for i, chl in enumerate(seg[0].segments[s].spiketrains):\n",
    "            spk_all[i] = np.append(spk_all[i],[seg[0].segments[s].spiketrains[i].times])\n",
    "            \n",
    "        csc_all_mag = np.vstack([csc_all_mag,seg[0].segments[s].analogsignals[0].magnitude])\n",
    "        csc_all_time = np.append(csc_all_time,seg[0].segments[s].analogsignals[0].times)\n",
    "        \n",
    "        for i, chl in enumerate(seg[0].segments[s].events):\n",
    "            beh_all[i] = np.append(beh_all[i],seg[0].segments[s].events[i].times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add data to nwb file\n",
    "from pynwb.ecephys import ElectricalSeries\n",
    "from pynwb.ecephys import SpikeEventSeries\n",
    "from pynwb.ecephys import EventWaveform\n",
    "from pynwb.behavior import BehavioralTimeSeries\n",
    "\n",
    "# add .ntt files\n",
    "ephys_waveform = EventWaveform()\n",
    "\n",
    "# loop through .ntt files\n",
    "for i, chl in enumerate(reader.header['unit_channels']):\n",
    "    \n",
    "    # get tetrode id\n",
    "    tetrode = re.search('(?<=TT)(.*?)(?=#)', chl[0]).group(0)\n",
    "    tetrode_name = 'TT' + tetrode\n",
    "           \n",
    "    if tetrode_name not in ephys_waveform.spike_event_series: # make tetrode if does not exist\n",
    "        \n",
    "        chl_list = []\n",
    "        \n",
    "        for j, group in enumerate(nwbfile.electrodes['group']):\n",
    "        \n",
    "            if tetrode in nwbfile.electrodes['group'][j].fields['description']:\n",
    "                \n",
    "                chl_list.append(j)\n",
    "        \n",
    "        electrode_table_region = nwbfile.create_electrode_table_region(chl_list, tetrode_name)\n",
    "        \n",
    "        for s in range(reader.header['nb_segment'][0]):\n",
    "            \n",
    "            if s is 0:\n",
    "                \n",
    "                waveform = reader.get_spike_raw_waveforms(seg_index=s, unit_index=i)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                waveform = np.vstack([waveform,reader.get_spike_raw_waveforms(seg_index=s, unit_index=i)])\n",
    "\n",
    "        ephys_waveform.create_spike_event_series(tetrode_name,\n",
    "                                                 waveform,\n",
    "                                                 spk_all[i],\n",
    "                                                 electrode_table_region)\n",
    "\n",
    "nwbfile.add_acquisition(ephys_waveform)\n",
    "\n",
    "# add .ncs files\n",
    "\n",
    "chl_list = []\n",
    "\n",
    "for chl in reader.header['signal_channels']['id']:\n",
    "    \n",
    "    chl_list.append(nwbfile.electrodes['id'][:].index(chl))\n",
    "    \n",
    "electrode_table_region = nwbfile.create_electrode_table_region(chl_list, 'CSC order for time series')\n",
    "\n",
    "ephys_ts = ElectricalSeries('CSC data',\n",
    "                            csc_all_mag,\n",
    "                            electrode_table_region,\n",
    "                            timestamps=csc_all_time,\n",
    "                            comments='n/a',\n",
    "                            description='unfiltered CSC data')\n",
    "\n",
    "nwbfile.add_acquisition(ephys_ts)\n",
    "\n",
    "# nwbfile.add_unit(id=1, electrodes=[0])\n",
    "# nwbfile.add_unit(id=2, electrodes=[0])\n",
    "\n",
    "# add .evt file\n",
    "\n",
    "beh_ts = BehavioralTimeSeries()\n",
    "\n",
    "# loop through events\n",
    "for i, chl in enumerate(reader.header['event_channels']):\n",
    "        \n",
    "    beh_ts.create_timeseries(str(chl), timestamps = beh_all[i])\n",
    "\n",
    "nwbfile.add_acquisition(beh_ts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the generated file\n",
    "from pynwb import NWBHDF5IO\n",
    "\n",
    "# TODO: I think we should right away use dandi-cli provided API to create the filename based on metadata\n",
    "# in the NWBFile\n",
    "with NWBHDF5IO('BCD_example.nwb', 'w') as io:\n",
    "    io.write(nwbfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
